{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a33598f-cf8c-4bc7-bcd1-2bf951afcf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Optimize CPU Usage\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4df28960-a4af-4442-9c5e-db3966a7f03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After SMOTE Balancing:\n",
      "SepsisLabel\n",
      "0    62.500005\n",
      "1    37.499995\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Training TabNet Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sinha\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.60833 | val_0_auc: 0.79257 |  0:02:17s\n",
      "epoch 1  | loss: 0.50219 | val_0_auc: 0.8334  |  0:04:38s\n",
      "epoch 2  | loss: 0.45859 | val_0_auc: 0.85195 |  0:07:02s\n",
      "epoch 3  | loss: 0.43371 | val_0_auc: 0.85836 |  0:09:10s\n",
      "epoch 4  | loss: 0.41918 | val_0_auc: 0.85631 |  0:11:16s\n",
      "epoch 5  | loss: 0.4102  | val_0_auc: 0.86561 |  0:13:17s\n",
      "epoch 6  | loss: 0.40464 | val_0_auc: 0.86576 |  0:15:21s\n",
      "epoch 7  | loss: 0.39943 | val_0_auc: 0.82165 |  0:17:31s\n",
      "epoch 8  | loss: 0.39571 | val_0_auc: 0.86307 |  0:19:35s\n",
      "epoch 9  | loss: 0.39196 | val_0_auc: 0.8708  |  0:21:33s\n",
      "epoch 10 | loss: 0.38867 | val_0_auc: 0.87594 |  0:23:31s\n",
      "epoch 11 | loss: 0.38688 | val_0_auc: 0.86861 |  0:25:29s\n",
      "epoch 12 | loss: 0.38431 | val_0_auc: 0.7725  |  0:27:26s\n",
      "epoch 13 | loss: 0.382   | val_0_auc: 0.85446 |  0:29:23s\n",
      "epoch 14 | loss: 0.38053 | val_0_auc: 0.88066 |  0:31:32s\n",
      "epoch 15 | loss: 0.3784  | val_0_auc: 0.8806  |  0:33:33s\n",
      "epoch 16 | loss: 0.37583 | val_0_auc: 0.85122 |  0:35:33s\n",
      "epoch 17 | loss: 0.37432 | val_0_auc: 0.87758 |  0:37:35s\n",
      "epoch 18 | loss: 0.37252 | val_0_auc: 0.85795 |  0:39:42s\n",
      "epoch 19 | loss: 0.37072 | val_0_auc: 0.88742 |  0:41:48s\n",
      "epoch 20 | loss: 0.3691  | val_0_auc: 0.84222 |  0:43:46s\n",
      "epoch 21 | loss: 0.36786 | val_0_auc: 0.82038 |  0:45:44s\n",
      "epoch 22 | loss: 0.3658  | val_0_auc: 0.87266 |  0:47:41s\n",
      "epoch 23 | loss: 0.36449 | val_0_auc: 0.83432 |  0:49:38s\n",
      "epoch 24 | loss: 0.36326 | val_0_auc: 0.85623 |  0:51:35s\n",
      "epoch 25 | loss: 0.36143 | val_0_auc: 0.76837 |  0:53:31s\n",
      "epoch 26 | loss: 0.35985 | val_0_auc: 0.78018 |  0:55:27s\n",
      "epoch 27 | loss: 0.35801 | val_0_auc: 0.83596 |  0:57:22s\n",
      "epoch 28 | loss: 0.35697 | val_0_auc: 0.81931 |  0:59:17s\n",
      "epoch 29 | loss: 0.3553  | val_0_auc: 0.81904 |  1:01:13s\n",
      "epoch 30 | loss: 0.35334 | val_0_auc: 0.84285 |  1:03:08s\n",
      "epoch 31 | loss: 0.35267 | val_0_auc: 0.88659 |  1:05:14s\n",
      "epoch 32 | loss: 0.35116 | val_0_auc: 0.87564 |  1:07:34s\n",
      "epoch 33 | loss: 0.3489  | val_0_auc: 0.88797 |  1:09:15s\n",
      "epoch 34 | loss: 0.34747 | val_0_auc: 0.798   |  1:11:01s\n",
      "epoch 35 | loss: 0.34604 | val_0_auc: 0.83998 |  1:12:51s\n",
      "epoch 36 | loss: 0.34411 | val_0_auc: 0.78042 |  1:14:42s\n",
      "epoch 37 | loss: 0.34245 | val_0_auc: 0.90082 |  1:16:33s\n",
      "epoch 38 | loss: 0.34076 | val_0_auc: 0.90264 |  1:18:23s\n",
      "epoch 39 | loss: 0.33937 | val_0_auc: 0.87898 |  1:20:13s\n",
      "epoch 40 | loss: 0.33798 | val_0_auc: 0.78259 |  1:22:03s\n",
      "epoch 41 | loss: 0.33618 | val_0_auc: 0.79049 |  1:23:53s\n",
      "epoch 42 | loss: 0.33487 | val_0_auc: 0.81249 |  1:25:56s\n",
      "epoch 43 | loss: 0.33372 | val_0_auc: 0.88133 |  1:27:51s\n",
      "epoch 44 | loss: 0.33187 | val_0_auc: 0.82516 |  1:29:41s\n",
      "epoch 45 | loss: 0.33095 | val_0_auc: 0.87306 |  1:31:32s\n",
      "epoch 46 | loss: 0.32934 | val_0_auc: 0.80432 |  1:33:23s\n",
      "epoch 47 | loss: 0.32807 | val_0_auc: 0.7206  |  1:35:13s\n",
      "epoch 48 | loss: 0.32638 | val_0_auc: 0.75643 |  1:37:04s\n",
      "epoch 49 | loss: 0.32507 | val_0_auc: 0.72501 |  1:38:54s\n",
      "epoch 50 | loss: 0.32308 | val_0_auc: 0.83253 |  1:40:41s\n",
      "epoch 51 | loss: 0.32158 | val_0_auc: 0.7493  |  1:42:47s\n",
      "epoch 52 | loss: 0.32012 | val_0_auc: 0.81484 |  1:44:36s\n",
      "epoch 53 | loss: 0.31937 | val_0_auc: 0.91308 |  1:46:21s\n",
      "epoch 54 | loss: 0.3175  | val_0_auc: 0.90513 |  1:48:10s\n",
      "epoch 55 | loss: 0.3166  | val_0_auc: 0.87717 |  1:50:01s\n",
      "epoch 56 | loss: 0.31526 | val_0_auc: 0.81053 |  1:51:50s\n",
      "epoch 57 | loss: 0.31427 | val_0_auc: 0.79033 |  1:53:38s\n",
      "epoch 58 | loss: 0.31305 | val_0_auc: 0.78356 |  1:55:27s\n",
      "epoch 59 | loss: 0.31167 | val_0_auc: 0.84531 |  1:57:15s\n",
      "epoch 60 | loss: 0.31012 | val_0_auc: 0.89657 |  1:59:05s\n",
      "epoch 61 | loss: 0.3086  | val_0_auc: 0.81863 |  2:00:54s\n",
      "epoch 62 | loss: 0.30778 | val_0_auc: 0.78783 |  2:02:43s\n",
      "epoch 63 | loss: 0.30666 | val_0_auc: 0.74757 |  2:04:32s\n",
      "epoch 64 | loss: 0.30528 | val_0_auc: 0.79731 |  2:06:40s\n",
      "epoch 65 | loss: 0.30455 | val_0_auc: 0.79343 |  2:08:30s\n",
      "epoch 66 | loss: 0.30324 | val_0_auc: 0.79039 |  2:10:19s\n",
      "epoch 67 | loss: 0.30209 | val_0_auc: 0.78641 |  2:12:09s\n",
      "epoch 68 | loss: 0.3012  | val_0_auc: 0.84611 |  2:13:57s\n",
      "epoch 69 | loss: 0.30101 | val_0_auc: 0.76847 |  2:15:46s\n",
      "epoch 70 | loss: 0.29903 | val_0_auc: 0.81809 |  2:17:36s\n",
      "epoch 71 | loss: 0.29813 | val_0_auc: 0.87588 |  2:19:25s\n",
      "epoch 72 | loss: 0.29699 | val_0_auc: 0.79522 |  2:21:15s\n",
      "epoch 73 | loss: 0.29618 | val_0_auc: 0.78967 |  2:23:04s\n",
      "\n",
      "Early stopping occurred at epoch 73 with best_epoch = 53 and best_val_0_auc = 0.91308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sinha\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Training Completed!\n",
      "Successfully saved model at tabnet_optuna.zip\n",
      "\n",
      "Optimized Model Saved Successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "\n",
    "# Load Preprocessed Dataset\n",
    "df = pd.read_csv(\"ProcessedData.csv\")\n",
    "\n",
    "# Split into Features & Labels\n",
    "X = df.drop(columns=['SepsisLabel'])\n",
    "y = df['SepsisLabel']\n",
    "\n",
    "# Apply SMOTE for Class Balancing (Increase Sepsis Cases to 60%)\n",
    "smote = SMOTE(sampling_strategy=0.6, random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Split into Train & Test Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.2, stratify=y_resampled, random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nAfter SMOTE Balancing:\")\n",
    "print(pd.Series(y_resampled).value_counts(normalize=True) * 100)\n",
    "\n",
    "\n",
    "# Optimized TabNet Model\n",
    "tabnet = TabNetClassifier(\n",
    "    optimizer_fn=torch.optim.AdamW,\n",
    "    optimizer_params=dict(lr=0.000400988413449954, weight_decay=0.0013603924614597604),\n",
    "    scheduler_params={\"step_size\":10, \"gamma\":0.9},  \n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    n_d=24,\n",
    "    n_a=56,\n",
    "    n_steps=3,\n",
    "    gamma=1.2,\n",
    "    mask_type=\"sparsemax\",\n",
    "    device_name='cuda'\n",
    ")\n",
    "\n",
    "print(\"\\nTraining TabNet Model...\")\n",
    "\n",
    "# Train the Model with Optimized Settings\n",
    "tabnet.fit(\n",
    "    X_train=X_train.values, y_train=y_train,\n",
    "    eval_set=[(X_test.values, y_test)],\n",
    "    eval_metric=[\"auc\"],\n",
    "    max_epochs=200,\n",
    "    patience=20,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=256\n",
    ")\n",
    "\n",
    "print(\"\\nModel Training Completed!\")\n",
    "\n",
    "# Save the Updated Model\n",
    "tabnet.save_model(\"tabnet_optuna\")\n",
    "print(\"\\nOptimized Model Saved Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a5523f2-584f-4fca-aa98-fc360533bf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.8462\n",
      "AUC-ROC Score: 0.8356\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88    292567\n",
      "           1       0.80      0.79      0.79    175540\n",
      "\n",
      "    accuracy                           0.85    468107\n",
      "   macro avg       0.84      0.84      0.84    468107\n",
      "weighted avg       0.85      0.85      0.85    468107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = tabnet.predict(X_test.values)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Final Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"AUC-ROC Score: {roc_auc_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf404425-8728-46d4-b0d8-9c1731c0eeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Device: CUDA\n",
      "\n",
      "TabNet Model Loaded Successfully!\n",
      "\n",
      "New Data Loaded: 50 samples for prediction.\n",
      "\n",
      "Probability Analysis:\n",
      "  - Min Probability: 0.0128\n",
      "  - Max Probability: 0.9953\n",
      "  - Mean Probability: 0.3965\n",
      "  - Sample Probabilities: [0.5268043  0.5452135  0.1396937  0.16169927 0.17242578 0.48984024\n",
      " 0.8126659  0.26892436 0.5107759  0.43663484 0.4324939  0.79088503\n",
      " 0.9952865  0.2570397  0.29459655 0.2514579  0.25212032 0.37399843\n",
      " 0.52422655 0.01279105]\n",
      "\n",
      "🔍 Dynamic Classification Threshold: 0.4839\n",
      "\n",
      "🔍 Class Distribution in Predictions:\n",
      "Sepsis_Prediction\n",
      "0    70.0\n",
      "1    30.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Predictions Completed!\n",
      "\n",
      "Results saved in 'Results.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sinha\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import os\n",
    "\n",
    "# ✅ Load Feature Names from Training Data\n",
    "training_data_path = \"ProcessedData.csv\"\n",
    "df_train = pd.read_csv(training_data_path)\n",
    "feature_columns = [col for col in df_train.columns if col != 'SepsisLabel']\n",
    "\n",
    "# ✅ Check GPU Availability\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"\\nUsing Device: {device.upper()}\")\n",
    "\n",
    "# ✅ Load Trained TabNet Model\n",
    "def load_tabnet_model(model_path=\"tabnet_sepsis_optimized.zip\"):\n",
    "    model = TabNetClassifier(device_name=device)  \n",
    "    model.load_model(model_path)\n",
    "    print(\"\\nTabNet Model Loaded Successfully!\")\n",
    "    return model\n",
    "\n",
    "# ✅ Load & Preprocess New Data\n",
    "def load_new_data(file_path=\"Synthetic_TestData.csv\"):\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Error: {file_path} not found!\")\n",
    "\n",
    "    # Load new patient data\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Ensure all expected columns exist and are in correct order\n",
    "    missing_features = set(feature_columns) - set(df.columns)\n",
    "    extra_features = set(df.columns) - set(feature_columns)\n",
    "\n",
    "    if missing_features:\n",
    "        raise ValueError(f\"Missing features in test data: {missing_features}\")\n",
    "    if extra_features:\n",
    "        print(f\"Warning: Extra features detected: {extra_features}. These will be ignored.\")\n",
    "        df = df[feature_columns]  # Keep only the expected features\n",
    "\n",
    "    print(f\"\\nNew Data Loaded: {df.shape[0]} samples for prediction.\")\n",
    "    return df\n",
    "\n",
    "# ✅ Run Predictions\n",
    "def predict_sepsis(model, new_data):\n",
    "    X_test = new_data.values\n",
    "    y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Probability Analysis\n",
    "    print(\"\\nProbability Analysis:\")\n",
    "    print(f\"  - Min Probability: {y_pred_prob.min():.4f}\")\n",
    "    print(f\"  - Max Probability: {y_pred_prob.max():.4f}\")\n",
    "    print(f\"  - Mean Probability: {y_pred_prob.mean():.4f}\")\n",
    "    print(f\"  - Sample Probabilities: {y_pred_prob[:20]}\")  # Print more samples\n",
    "\n",
    "    # ✅ Dynamic Thresholding based on probability distribution\n",
    "    threshold = np.percentile(y_pred_prob, 70)  # Adjust based on distribution\n",
    "\n",
    "    print(f\"\\n🔍 Dynamic Classification Threshold: {threshold:.4f}\")\n",
    "\n",
    "    # Apply dynamic threshold\n",
    "    y_pred = (y_pred_prob > threshold).astype(int)\n",
    "\n",
    "    # Add Predictions to DataFrame\n",
    "    new_data[\"Sepsis_Prediction\"] = y_pred\n",
    "    new_data[\"Sepsis_Probability\"] = y_pred_prob\n",
    "\n",
    "    # Class Distribution\n",
    "    print(\"\\n🔍 Class Distribution in Predictions:\")\n",
    "    print(new_data[\"Sepsis_Prediction\"].value_counts(normalize=True) * 100)\n",
    "\n",
    "    print(\"\\nPredictions Completed!\")\n",
    "    return new_data\n",
    "\n",
    "# ✅ Save Predictions to \"Results.csv\"\n",
    "def save_predictions(predictions, output_file=\"Results.csv\"):\n",
    "    predictions.to_csv(output_file, index=False)\n",
    "    print(f\"\\nResults saved in '{output_file}'.\")\n",
    "\n",
    "# ✅ Execute Inference Pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    model = load_tabnet_model(\"tabnet_sepsis_optimized.zip\")  \n",
    "    new_data = load_new_data(\"Synthetic_TestData.csv\")  \n",
    "    predictions = predict_sepsis(model, new_data)  \n",
    "    save_predictions(predictions, \"Results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13e24ed9-72ed-4d4a-a659-351aa6c255aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Device: CUDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sinha\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TabNet Model Loaded Successfully!\n",
      "\n",
      "Final Accuracy: 0.8462\n",
      "AUC-ROC Score: 0.9131\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88    292567\n",
      "           1       0.80      0.79      0.79    175540\n",
      "\n",
      "    accuracy                           0.85    468107\n",
      "   macro avg       0.84      0.84      0.84    468107\n",
      "weighted avg       0.85      0.85      0.85    468107\n",
      "\n",
      "\n",
      "Test Results Saved in 'Test_Results.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import os\n",
    "\n",
    "# ✅ Set Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"\\nUsing Device: {device.upper()}\")\n",
    "\n",
    "# ✅ Load Processed Dataset\n",
    "data_path = \"ProcessedData.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# ✅ Split into Features & Labels\n",
    "X = df.drop(columns=['SepsisLabel'])\n",
    "y = df['SepsisLabel']\n",
    "\n",
    "# ✅ Apply SMOTE for Class Balancing\n",
    "smote = SMOTE(sampling_strategy=0.6, random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# ✅ Split into Train & Test Sets (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.2, stratify=y_resampled, random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Load Trained TabNet Model\n",
    "def load_tabnet_model(model_path=\"tabnet_optuna.zip\"):\n",
    "    model = TabNetClassifier(device_name=device)\n",
    "    model.load_model(model_path)\n",
    "    print(\"\\nTabNet Model Loaded Successfully!\")\n",
    "    return model\n",
    "\n",
    "# ✅ Run Predictions\n",
    "def predict_sepsis(model, X_test):\n",
    "    y_pred_prob = model.predict_proba(X_test.values)[:, 1]\n",
    "\n",
    "    # Convert probabilities to binary predictions\n",
    "    y_pred = (y_pred_prob > 0.50).astype(int)\n",
    "\n",
    "    return y_pred, y_pred_prob\n",
    "\n",
    "# ✅ Load Model & Predict\n",
    "model = load_tabnet_model(\"tabnet_optuna.zip\")\n",
    "y_pred, y_pred_prob = predict_sepsis(model, X_test)\n",
    "\n",
    "# ✅ Evaluate Model Performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "auc_roc = roc_auc_score(y_test, y_pred_prob)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nFinal Accuracy: {accuracy:.4f}\")\n",
    "print(f\"AUC-ROC Score: {auc_roc:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", class_report)\n",
    "\n",
    "# ✅ Save Predictions to CSV\n",
    "results_df = X_test.copy()\n",
    "results_df[\"SepsisLabel\"] = y_test.values\n",
    "results_df[\"Sepsis_Prediction\"] = y_pred\n",
    "results_df[\"Sepsis_Probability\"] = y_pred_prob\n",
    "\n",
    "results_df.to_csv(\"Test_Results.csv\", index=False)\n",
    "print(\"\\nTest Results Saved in 'Test_Results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aa1d23-dfe6-47e1-964a-4322e81d7cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
